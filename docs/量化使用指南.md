# llama.cpp 模型量化使用指南

## 简介

模型量化是将高精度模型（如 F32 或 BF16）转换为低精度格式（如 4 位整数）的过程。量化可以：
- **减小模型大小**：例如 8B 模型从 32.1 GB 减小到 4.9 GB（Q4_K_M）
- **加速推理**：量化后的模型推理速度更快
- **降低内存占用**：在资源受限的设备上运行更大的模型

注意：量化可能会带来一定的精度损失，但通常可以通过使用重要性矩阵（imatrix）来最小化这种损失。

## 前提条件

1. **已编译量化工具**：确保 `llama-quantize` 工具已编译
   ```bash
   # 如果还未编译，执行：
   cmake -B build -DCMAKE_BUILD_TYPE=Release
   cmake --build build --config Release --target llama-quantize -j$(nproc)
   ```

2. **准备模型文件**：需要 GGUF 格式的模型文件（通常是 F16 或 F32 格式）

## 基本用法

### 基本命令格式

```bash
./build/bin/llama-quantize [选项] 输入模型.gguf [输出模型.gguf] 量化类型 [线程数]
```

### 简单示例

```bash
# 使用 Q4_K_M 量化方法，使用 8 个 CPU 线程
# 如果不指定输出文件名，会自动生成（例如：输入文件名-Q4_K_M.gguf）
./build/bin/llama-quantize input-model-f16.gguf q4_k_m 8

# 指定输出文件名
./build/bin/llama-quantize input-model-f16.gguf output-model-q4_k_m.gguf Q4_K_M 8
```

## 量化类型选择

### 推荐的量化类型（按质量/大小平衡）

| 量化类型 | 每权重位数 | 8B 模型大小 | 特点 |
|---------|-----------|------------|------|
| **Q4_K_M** | 4.89 | 4.58 GiB | **推荐**：质量与大小的最佳平衡 |
| **Q5_K_M** | 5.70 | 5.33 GiB | 更高质量，稍大 |
| **Q3_K_M** | 4.00 | 3.74 GiB | 更小，质量略低 |
| **Q6_K** | 6.56 | 6.14 GiB | 高质量，接近 F16 |
| **Q8_0** | 8.50 | 7.95 GiB | 接近原始精度 |

### 极低比特量化（需要 imatrix）

| 量化类型 | 每权重位数 | 8B 模型大小 | 说明 |
|---------|-----------|------------|------|
| **IQ2_XS** | 2.59 | 2.42 GiB | 需要 imatrix |
| **IQ2_S** | 2.74 | 2.56 GiB | 需要 imatrix |
| **IQ3_XXS** | 3.25 | 3.04 GiB | 需要 imatrix |
| **IQ3_M** | 3.76 | 3.52 GiB | 需要 imatrix |

## 完整工作流程

### 1. 从 Hugging Face 转换模型

如果您有 Hugging Face 格式的模型，需要先转换为 GGUF 格式：

```bash
# 安装 Python 依赖
python3 -m pip install -r requirements.txt

# 转换模型（假设模型在 ./models/mymodel/ 目录）
python3 convert_hf_to_gguf.py ./models/mymodel/
```

### 2. 量化模型

```bash
# 量化到 Q4_K_M（推荐）
./build/bin/llama-quantize \
    ./models/mymodel/ggml-model-f16.gguf \
    ./models/mymodel/ggml-model-Q4_K_M.gguf \
    Q4_K_M 8
```

### 3. 运行量化后的模型

```bash
# 使用量化后的模型进行推理
./build/bin/llama-cli \
    -m ./models/mymodel/ggml-model-Q4_K_M.gguf \
    -p "你好，请介绍一下自己"
```

## 高级选项

### 使用重要性矩阵（推荐）

重要性矩阵可以显著提高量化质量，特别是对于低比特量化：

```bash
# 首先生成重要性矩阵（需要先运行 llama-imatrix）
# 然后使用 imatrix 进行量化
./build/bin/llama-quantize \
    --imatrix imatrix.gguf \
    input-model-f16.gguf \
    output-model-q4_k_m.gguf \
    Q4_K_M 8
```

### 自定义张量量化类型

```bash
# 设置输出层为 Q5_K，词嵌入层为 Q3_K_M
./build/bin/llama-quantize \
    --imatrix imatrix.gguf \
    --output-tensor-type q5_k \
    --token-embedding-type q3_k_m \
    input-model-f16.gguf \
    output-model.gguf \
    Q4_K_M 8
```

### 仅对特定张量使用重要性矩阵

```bash
# 仅对 attn_v 和 ffn_down 张量使用重要性矩阵
./build/bin/llama-quantize \
    --imatrix imatrix.gguf \
    --include-weights attn_v \
    --include-weights ffn_down \
    input-model-f16.gguf \
    output-model.gguf \
    Q4_K_M 8
```

### 使用正则表达式指定张量类型

```bash
# 将奇数层的 attn_k 量化为 Q5_K_M，偶数层的 attn_q 量化为 Q3_K_M
./build/bin/llama-quantize \
    --imatrix imatrix.gguf \
    --tensor-type "\.(\d*[13579])\.attn_k=q5_k" \
    --tensor-type "\.(\d*[02468])\.attn_q=q3_k" \
    input-model-f16.gguf \
    output-model.gguf \
    Q4_K_M 8
```

### 其他有用选项

```bash
# 允许重新量化已量化的模型（不推荐，质量会下降）
./build/bin/llama-quantize \
    --allow-requantize \
    input-model.gguf \
    output-model.gguf \
    Q4_K_M 8

# 保持输出层不量化（提高质量，但增加模型大小）
./build/bin/llama-quantize \
    --leave-output-tensor \
    input-model-f16.gguf \
    output-model.gguf \
    Q4_K_M 8

# 纯量化模式（所有张量使用相同类型，不使用 k-quant 混合）
./build/bin/llama-quantize \
    --pure \
    input-model-f16.gguf \
    output-model.gguf \
    Q4_K 8
```

## 内存和磁盘要求

量化过程需要将整个模型加载到内存中，因此需要：

- **足够的 RAM**：至少与模型大小相同
- **足够的磁盘空间**：用于存储输入和输出模型

示例（Llama 3.1）：

| 模型大小 | 原始大小 | Q4_K_M 量化后 |
|---------|---------|--------------|
| 8B | 32.1 GB | 4.9 GB |
| 70B | 280.9 GB | 43.1 GB |
| 405B | 1,625.1 GB | 249.1 GB |

## 常见问题

### Q: 我应该选择哪种量化类型？

**A:** 
- **一般用途**：推荐 `Q4_K_M`，质量与大小的最佳平衡
- **追求质量**：使用 `Q5_K_M` 或 `Q6_K`
- **追求小体积**：使用 `Q3_K_M` 或 `IQ3_M`（需要 imatrix）
- **极致压缩**：使用 `IQ2_XS` 或 `IQ2_S`（需要 imatrix）

### Q: 是否需要重要性矩阵（imatrix）？

**A:** 
- **必须使用**：对于 IQ1_S, IQ1_M, IQ2_S, IQ2_XXS, IQ2_XS, Q2_K_S
- **强烈推荐**：对于所有量化类型，可以显著提高质量
- **可选**：对于 Q4_K_M 及以上，不使用也能获得不错的结果

### Q: 可以重新量化已量化的模型吗？

**A:** 可以，但不推荐。使用 `--allow-requantize` 选项，但质量会显著下降。最好从原始 F16/F32 模型开始量化。

### Q: 量化需要多长时间？

**A:** 取决于模型大小和 CPU 性能。通常 8B 模型需要几分钟到十几分钟。

## 性能对比参考

以 Llama 3.1 8B 模型为例（在 512 token 提示处理，128 token 生成）：

| 量化类型 | 提示处理 (t/s) | 文本生成 (t/s) |
|---------|--------------|---------------|
| F16 | 923.49 | 29.17 |
| Q8_0 | 865.09 | 50.93 |
| Q4_K_M | 821.81 | 71.93 |
| Q3_K_M | 783.44 | 71.68 |
| IQ2_XS | 826.99 | 78.04 |

## 更多资源

- 官方文档：`tools/quantize/README.md`
- 在线量化工具：https://huggingface.co/spaces/ggml-org/gguf-my-repo
- 构建文档：`docs/build.md`

## 快速开始示例

```bash
# 1. 确保工具已编译
cd /home/lab/Projects/llama.cpp
cmake -B build -DCMAKE_BUILD_TYPE=Release
cmake --build build --config Release --target llama-quantize -j$(nproc)

# 2. 量化模型（假设您已有 F16 格式的模型）
./build/bin/llama-quantize \
    ./models/your-model-f16.gguf \
    ./models/your-model-Q4_K_M.gguf \
    Q4_K_M 8

# 3. 验证量化结果
ls -lh ./models/your-model-Q4_K_M.gguf

# 4. 使用量化后的模型
./build/bin/llama-cli \
    -m ./models/your-model-Q4_K_M.gguf \
    -p "你好"
```

---

**提示**：如果您还没有模型文件，可以从 Hugging Face 下载并转换，或直接下载已量化的 GGUF 模型。

## Fun-Audio-Chat-8B 模型量化示例

针对 `/home/lab/Projects/Fun-Audio-Chat/pretrained_models/Fun-Audio-Chat-8B` 模型的完整量化流程：

### 步骤 1: 安装依赖

```bash
cd /home/lab/Projects/llama.cpp
python3 -m pip install -r requirements.txt
```

### 步骤 2: 编译量化工具

```bash
cmake -B build -DCMAKE_BUILD_TYPE=Release
cmake --build build --config Release --target llama-quantize -j$(nproc)
```

### 步骤 3: 转换 Hugging Face 模型为 GGUF 格式

由于该模型是 Hugging Face 格式（safetensors），需要先转换为 GGUF：

```bash
# 转换模型为 F16 格式的 GGUF
python3 convert_hf_to_gguf.py \
    --outfile /home/lab/Projects/Fun-Audio-Chat/pretrained_models/Fun-Audio-Chat-8B/Fun-Audio-Chat-8B-f16.gguf \
    --outtype f16 \
    /home/lab/Projects/Fun-Audio-Chat/pretrained_models/Fun-Audio-Chat-8B
```

**注意**：转换过程可能需要一些时间，并且需要足够的磁盘空间（约 16GB 用于 F16 格式）。

### 步骤 4: 量化模型

转换完成后，使用 `llama-quantize` 工具进行量化：

```bash
# 使用 Q4_K_M 量化（推荐，质量与大小的最佳平衡）
./build/bin/llama-quantize \
    /home/lab/Projects/Fun-Audio-Chat/pretrained_models/Fun-Audio-Chat-8B/Fun-Audio-Chat-8B-f16.gguf \
    /home/lab/Projects/Fun-Audio-Chat/pretrained_models/Fun-Audio-Chat-8B/Fun-Audio-Chat-8B-Q4_K_M.gguf \
    Q4_K_M 8
```

### 其他量化选项

如果您需要不同的量化级别：

```bash
# Q5_K_M - 更高质量，稍大（约 5.3 GiB）
./build/bin/llama-quantize \
    /home/lab/Projects/Fun-Audio-Chat/pretrained_models/Fun-Audio-Chat-8B/Fun-Audio-Chat-8B-f16.gguf \
    /home/lab/Projects/Fun-Audio-Chat/pretrained_models/Fun-Audio-Chat-8B/Fun-Audio-Chat-8B-Q5_K_M.gguf \
    Q5_K_M 8

# Q3_K_M - 更小，质量略低（约 3.7 GiB）
./build/bin/llama-quantize \
    /home/lab/Projects/Fun-Audio-Chat/pretrained_models/Fun-Audio-Chat-8B/Fun-Audio-Chat-8B-f16.gguf \
    /home/lab/Projects/Fun-Audio-Chat/pretrained_models/Fun-Audio-Chat-8B/Fun-Audio-Chat-8B-Q3_K_M.gguf \
    Q3_K_M 8
```

### 步骤 5: 验证量化结果

```bash
# 检查量化后的模型文件大小
ls -lh /home/lab/Projects/Fun-Audio-Chat/pretrained_models/Fun-Audio-Chat-8B/*.gguf
```

### 步骤 6: 使用量化后的模型

```bash
# 使用量化后的模型进行推理
./build/bin/llama-cli \
    -m /home/lab/Projects/Fun-Audio-Chat/pretrained_models/Fun-Audio-Chat-8B/Fun-Audio-Chat-8B-Q4_K_M.gguf \
    -p "你好，请介绍一下自己"
```

### 完整脚本示例

您也可以创建一个脚本来自动化整个过程：

```bash
#!/bin/bash
set -e

MODEL_DIR="/home/lab/Projects/Fun-Audio-Chat/pretrained_models/Fun-Audio-Chat-8B"
LLAMA_CPP_DIR="/home/lab/Projects/llama.cpp"
QUANT_TYPE="Q4_K_M"

cd "$LLAMA_CPP_DIR"

# 1. 转换模型
echo "步骤 1: 转换模型为 GGUF 格式..."
python3 convert_hf_to_gguf.py \
    --outfile "$MODEL_DIR/Fun-Audio-Chat-8B-f16.gguf" \
    --outtype f16 \
    "$MODEL_DIR"

# 2. 量化模型
echo "步骤 2: 量化模型..."
./build/bin/llama-quantize \
    "$MODEL_DIR/Fun-Audio-Chat-8B-f16.gguf" \
    "$MODEL_DIR/Fun-Audio-Chat-8B-$QUANT_TYPE.gguf" \
    $QUANT_TYPE 8

echo "量化完成！模型保存在: $MODEL_DIR/Fun-Audio-Chat-8B-$QUANT_TYPE.gguf"
```

### 注意事项

1. **磁盘空间**：确保有足够的磁盘空间（至少 20GB 用于转换和量化过程）
2. **内存要求**：量化过程需要将整个模型加载到内存，8B 模型至少需要 16GB RAM
3. **时间**：转换和量化过程可能需要 10-30 分钟，取决于您的硬件配置
4. **模型格式**：如果转换过程中遇到问题，请检查模型是否支持（某些特殊架构可能需要额外的转换参数）



