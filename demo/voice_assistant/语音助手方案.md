# 语音助手 Demo 方案文档

## 1. 项目概述

### 1.1 目标

在 `demo` 目录下实现一个**语音助手 Demo**，实现「语音输入 → 大模型理解与回复 → 语音输出」的完整闭环，用于验证本仓库的 LLM 能力在语音交互场景下的可用性。

### 1.2 核心能力

- **语音输入 (ASR)**：用户通过麦克风说话，转为文本。
- **对话理解与生成 (LLM)**：基于现有 agents 能力，使用 Qwen3 等模型理解意图并生成回复。
- **语音输出 (TTS)**：将模型回复的文本转为语音播放。

### 1.3 约束与原则

- 遵循项目规则：**仅使用 ollama、siliconflow、modelscope** 的 API，优先 **Qwen3** 系列模型。
- API Key 等敏感信息从 **`~/.zshrc` 环境变量** 读取。
- 使用 **uv** 管理 Python 依赖与运行环境。

---

## 2. 技术选型

### 2.1 大模型 (LLM)

| 方案        | 说明 |
|-------------|------|
| **Ollama**  | 本地部署，无 API Key，适合开发调试；模型如 `qwen3:0.6b`、`qwen3:8b`。 |
| **SiliconFlow** | 云端 API，需 `SILICONFLOW_API_KEY`，低延迟。 |
| **ModelScope** | 云端/本地均可，需按项目现有 agents 方式接入。 |

**建议**：Demo 默认与 `demo/agents/agent_system.py` 一致，通过环境变量 `USE_OLLAMA` / `SILICONFLOW_API_KEY` / `USE_MODELSCOPE` 切换，模型名统一用 Qwen3 系列。

### 2.2 语音识别 (ASR)

| 方案 | 说明 | 与项目规则一致性 |
|------|------|------------------|
| **ModelScope FunASR** | 阿里魔搭语音识别，支持流式/离线，Python SDK 完善。 | ✅ 符合（modelscope） |
| **Ollama + 语音** | 若后续 Ollama 提供语音接口可接入。 | ✅ 符合 |
| **本地 Whisper 等** | 可选备用方案，不依赖项目规定 API。 | 文档中标注为可选 |

**建议**：优先采用 **ModelScope FunASR** 作为 ASR，与「仅用 ollama/siliconflow/modelscope」原则一致；若魔搭暂无合适现网 API，可在文档中说明「先文本输入占位，后续接 FunASR」。

### 2.3 语音合成 (TTS)

| 方案 | 说明 | 与项目规则一致性 |
|------|------|------------------|
| **ModelScope TTS** | 魔搭语音合成模型，可与 FunASR 同生态。 | ✅ 符合（modelscope） |
| **Ollama 语音** | 若 Ollama 提供 TTS 能力则优先。 | ✅ 符合 |
| **本地/离线 TTS** | 如 pyttsx3、edge-tts 等仅作可选或开发占位。 | 文档标注可选 |

**建议**：优先 **ModelScope TTS**；若 Demo 第一阶段只做「文本输入 → LLM → 文本输出」，可先实现 TTS 占位（打印或简单播放），第二阶段再接 ModelScope TTS。

---

## 3. 系统架构

### 3.1 数据流

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│  麦克风     │ ──▶ │  ASR        │ ──▶ │  LLM        │ ──▶ │  TTS        │ ──▶ │  扬声器     │
│  用户说话   │     │  语音→文本   │     │  理解与回复  │     │  文本→语音   │     │  播放回复   │
└─────────────┘     └─────────────┘     └─────────────┘     └─────────────┘     └─────────────┘
```

### 3.2 模块职责

| 模块 | 职责 | 复用/依赖 |
|------|------|-----------|
| **ASR 模块** | 录音、端点检测（可选）、调用 ASR API 得到文本。 | 新增；API 优先 ModelScope FunASR。 |
| **对话模块** | 维护会话历史，调用 agents 的 LLM 生成回复。 | 复用 `demo/agents/agent_system.py` 或其封装。 |
| **TTS 模块** | 将回复文本送入 TTS API，得到音频并播放。 | 新增；API 优先 ModelScope TTS。 |
| **Demo 主控** | 串联 ASR → LLM → TTS，支持单轮/多轮（可选）。 | 新增于本子项目。 |

### 3.3 目录结构（已实现）

```
demo/
├── README.md                    # 总览：多项目说明
└── voice_assistant/             # 子项目：语音助手
    ├── README.md
    ├── 语音助手方案.md
    ├── run_voice_assistant.py
    └── voice_assistant/         # Python 实现包
        ├── __init__.py
        ├── llm_client.py
        ├── pipeline.py
        ├── asr.py
        └── tts.py
```

阶段一已实现：`llm_client` + `pipeline` 文本模式；`asr.py`、`tts.py` 为占位，阶段二接入真实 ASR/TTS。

---

## 4. 环境与依赖

### 4.1 环境变量（与现有项目一致）

- `USE_OLLAMA`：是否使用 Ollama（如 `true`）。
- `OLLAMA_BASE_URL`：Ollama 服务地址（默认 `http://localhost:11434`）。
- `SILICONFLOW_API_KEY`：硅基流动 API Key（从 `~/.zshrc` 等加载）。
- `USE_MODELSCOPE`：是否使用 ModelScope 本地/管道。
- ModelScope / FunASR / TTS 所需 Key 或 Token（若魔搭要求），建议也从环境变量读取。

### 4.2 依赖

- 现有 `pyproject.toml` 已包含 langchain、ollama、requests、modelscope 等。
- Demo 若用 ModelScope FunASR / TTS，需在 `pyproject.toml` 或 `demo/requirements-demo.txt` 中增加对应 SDK（如 `modelscope` 的语音相关依赖、音频处理库如 `sounddevice`/`pyaudio`、`numpy` 等）。
- 录音与播放：可选 `sounddevice`、`pyaudio`、`scipy`（写 wav）等，由方案文档注明，实现时再定具体库。

---

## 5. 实现阶段建议

### 阶段一：文本版「伪语音助手」（优先）

- **目标**：不依赖麦克风与扬声器，先跑通「用户文本 → LLM → 回复文本」。
- **内容**：
  - 在 `demo/voice_assistant/` 下实现 `llm_client`，封装对 `agents` 的调用（与现有 agent_system 一致，使用 Qwen3）。
  - 实现简单 `pipeline`：从标准输入读一行文本 → 调用 LLM → 打印回复。
  - 编写 `run_voice_assistant.py` 文本模式入口。
- **产出**：可在终端用文本与「语音助手逻辑」对话，验证 LLM 与项目配置无误。

### 阶段二：接入真实语音

- **目标**：ASR + TTS 接入，实现完整语音闭环。
- **内容**：
  - 集成 ModelScope FunASR（或项目允许的 ASR 方案），实现 `asr.py`；从麦克风或音频文件得到文本。
  - 集成 ModelScope TTS（或项目允许的 TTS 方案），实现 `tts.py`；将回复文本转为音频并播放。
  - 在 `pipeline` 中串联：录音 → ASR → LLM → TTS → 播放。
- **产出**：真实「说话 → 听回复」的语音助手 Demo。

### 阶段三：体验优化（可选）

- 多轮对话：维护会话历史，支持上下文连续对话。
- 唤醒词 / 按键说话：简化交互方式。
- 流式 TTS：边生成边播，降低首包延迟。

---

## 6. 风险与备选

- **ModelScope 语音 API 变更**：若 FunASR/TTS 接口或鉴权方式与文档不一致，以官方文档为准，并在本目录 README 中注明当前所用版本与示例。
- **设备与权限**：麦克风/扬声器需要系统权限，建议在 README 中说明运行环境（如 macOS/Linux）与权限要求。
- **离线与延迟**：Ollama 本地 LLM 延迟较低；云端 SiliconFlow/ModelScope 需考虑网络延迟，可在文档中给出「推荐本地开发用 Ollama」的说明。

---

## 7. 验收标准（方案阶段）

- [ ] 方案文档通过评审，技术选型符合项目规则（仅 ollama / siliconflow / modelscope，优先 Qwen3）。
- [ ] 目录结构与模块划分清晰，便于先做文本版再接语音。
- [ ] 环境变量与依赖说明完整，后续实现可按文档执行。

---

**文档版本**：v0.1  
**最后更新**：2025-01-31
