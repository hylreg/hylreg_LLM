# 魔搭（ModelScope）模型下载指南

## 简介

魔搭（ModelScope）是阿里巴巴推出的模型社区平台，提供了丰富的开源模型资源。通过 ModelScope 可以方便地下载和使用各种预训练模型，包括：

- **大语言模型（LLM）**：如 Qwen、ChatGLM、Baichuan 等
- **嵌入模型（Embedding）**：用于文本向量化
- **多模态模型**：支持图像、音频等多种输入
- **领域专用模型**：如代码生成、数学推理等

## 前提条件

### 1. 安装 ModelScope SDK

```bash
# 使用 pip 安装
pip install modelscope

# 或使用 uv 安装（推荐）
uv pip install modelscope
```

### 2. 安装 Git LFS（可选但推荐）

对于大型模型文件，ModelScope 使用 Git LFS 进行存储。安装 Git LFS 可以更高效地下载大文件：

```bash
# Ubuntu/Debian
sudo apt-get install git-lfs

# macOS
brew install git-lfs

# 初始化 Git LFS
git lfs install
```

### 3. 配置认证（可选）

某些模型可能需要认证才能下载。您可以在 [ModelScope 官网](https://www.modelscope.cn/) 注册账号并获取 token：

```bash
# 设置环境变量（推荐，统一使用 API_KEY 命名风格）
export MODELSCOPE_API_KEY="your-token-here"

# 或在代码中设置
from modelscope.hub.api import HubApi
api = HubApi()
api.login('your-token-here')
```

## 基本用法

### 方式一：使用 snapshot_download（推荐）

这是下载完整模型的最简单方法：

```python
from modelscope import snapshot_download

# 下载模型到本地
model_dir = snapshot_download(
    'qwen/Qwen-7B-Chat',
    cache_dir='./models'
)

print(f"模型已下载到: {model_dir}")
```

### 方式二：使用 Model.from_pretrained

直接加载模型（会自动下载）：

```python
from modelscope import AutoModel, AutoTokenizer

# 自动下载并加载模型
model = AutoModel.from_pretrained(
    'qwen/Qwen-7B-Chat',
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(
    'qwen/Qwen-7B-Chat',
    trust_remote_code=True
)
```

### 方式三：使用命令行工具

```bash
# 安装命令行工具
pip install modelscope[cli]

# 下载模型
modelscope download qwen/Qwen-7B-Chat --cache_dir ./models
```

## 常用模型下载示例

### 1. Qwen 系列模型

```python
from modelscope import snapshot_download

# Qwen 7B Chat 模型
qwen_7b = snapshot_download(
    'qwen/Qwen-7B-Chat',
    cache_dir='./models/qwen'
)

# Qwen 14B Chat 模型
qwen_14b = snapshot_download(
    'qwen/Qwen-14B-Chat',
    cache_dir='./models/qwen'
)

# Qwen 1.8B 基础模型
qwen_1_8b = snapshot_download(
    'qwen/Qwen-1_8B',
    cache_dir='./models/qwen'
)
```

### 2. 嵌入模型（Embedding）

```python
from modelscope import snapshot_download

# Qwen 嵌入模型
qwen_embedding = snapshot_download(
    'damo/nlp_gte_sentence-embedding_chinese-base',
    cache_dir='./models/embeddings'
)

# 通用嵌入模型
general_embedding = snapshot_download(
    'damo/nlp_gte_sentence-embedding_chinese-large',
    cache_dir='./models/embeddings'
)
```

### 3. ChatGLM 系列

```python
from modelscope import snapshot_download

# ChatGLM3-6B
chatglm3 = snapshot_download(
    'ZhipuAI/chatglm3-6b',
    cache_dir='./models/chatglm'
)
```

### 4. 多模态模型

```python
from modelscope import snapshot_download

# Qwen-VL 视觉语言模型
qwen_vl = snapshot_download(
    'qwen/Qwen-VL-Chat',
    cache_dir='./models/qwen-vl'
)
```

## 高级选项

### 指定下载路径

```python
from modelscope import snapshot_download

model_dir = snapshot_download(
    'qwen/Qwen-7B-Chat',
    cache_dir='/path/to/your/models',  # 自定义缓存目录
    local_dir='./local_qwen',          # 本地目录（可选）
    revision='master'                   # 指定版本/分支
)
```

### 仅下载特定文件

```python
from modelscope import snapshot_download

# 只下载配置文件，不下载模型权重
model_dir = snapshot_download(
    'qwen/Qwen-7B-Chat',
    cache_dir='./models',
    ignore_file_pattern=['*.safetensors', '*.bin']  # 忽略模型权重文件
)
```

### 使用代理（如果需要）

```python
import os
from modelscope import snapshot_download

# 设置代理环境变量
os.environ['HTTP_PROXY'] = 'http://your-proxy:port'
os.environ['HTTPS_PROXY'] = 'http://your-proxy:port'

model_dir = snapshot_download(
    'qwen/Qwen-7B-Chat',
    cache_dir='./models'
)
```

### 断点续传

ModelScope SDK 默认支持断点续传。如果下载中断，重新运行相同的下载命令会自动从中断处继续：

```python
from modelscope import snapshot_download

# 如果之前下载中断，重新运行会自动续传
model_dir = snapshot_download(
    'qwen/Qwen-7B-Chat',
    cache_dir='./models'
)
```

## 与 Hugging Face 兼容性

ModelScope 的模型通常与 Hugging Face 格式兼容。下载后可以直接使用 Hugging Face 的 `transformers` 库加载：

```python
from transformers import AutoModel, AutoTokenizer

# 使用 ModelScope 下载的模型路径
model_path = './models/qwen/Qwen-7B-Chat'

# 使用 Hugging Face transformers 加载
model = AutoModel.from_pretrained(model_path, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
```

## 常见问题

### Q: 下载速度慢怎么办？

**A:** 
1. 使用国内镜像源（ModelScope 默认使用国内 CDN，速度较快）
2. 安装 Git LFS 以提高大文件下载效率
3. 使用代理（如果在海外）
4. 选择合适的时间段下载（避开高峰期）

### Q: 如何查看模型文件大小？

**A:** 
```python
from modelscope import snapshot_download
import os

model_dir = snapshot_download('qwen/Qwen-7B-Chat', cache_dir='./models')

# 计算目录大小
def get_dir_size(path):
    total = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            total += os.path.getsize(filepath)
    return total / (1024**3)  # 转换为 GB

size_gb = get_dir_size(model_dir)
print(f"模型大小: {size_gb:.2f} GB")
```

### Q: 下载失败怎么办？

**A:** 
1. 检查网络连接
2. 确认模型名称是否正确
3. 检查磁盘空间是否充足
4. 尝试使用 `revision='master'` 指定版本
5. 清除缓存后重试：
   ```python
   from modelscope.hub.file_download import model_file_download
   # 删除缓存目录后重新下载
   ```

### Q: 如何只下载模型配置文件？

**A:** 
```python
from modelscope import snapshot_download

# 忽略大文件，只下载配置文件
model_dir = snapshot_download(
    'qwen/Qwen-7B-Chat',
    cache_dir='./models',
    ignore_file_pattern=['*.safetensors', '*.bin', '*.pt', '*.pth']
)
```

### Q: 下载的模型在哪里？

**A:** 
默认情况下，模型会下载到：
- Linux/macOS: `~/.cache/modelscope/hub/`
- Windows: `C:\Users\<username>\.cache\modelscope\hub\`

您可以通过 `cache_dir` 参数自定义路径。

### Q: 如何更新已下载的模型？

**A:** 
```python
from modelscope import snapshot_download

# 指定 revision 或删除缓存目录后重新下载
model_dir = snapshot_download(
    'qwen/Qwen-7B-Chat',
    cache_dir='./models',
    revision='master'  # 或指定具体的 commit hash
)
```

## 完整示例脚本

```python
#!/usr/bin/env python3
"""
ModelScope 模型下载示例脚本
"""

from modelscope import snapshot_download
import os

def download_model(model_name, cache_dir='./models', revision='master'):
    """
    下载 ModelScope 模型
    
    Args:
        model_name: 模型名称，格式为 'namespace/model-name'
        cache_dir: 缓存目录
        revision: 模型版本/分支
    """
    print(f"开始下载模型: {model_name}")
    print(f"缓存目录: {cache_dir}")
    
    try:
        model_dir = snapshot_download(
            model_name,
            cache_dir=cache_dir,
            revision=revision
        )
        print(f"✓ 模型下载成功！")
        print(f"  路径: {model_dir}")
        return model_dir
    except Exception as e:
        print(f"✗ 下载失败: {e}")
        return None

# 示例：下载常用模型
if __name__ == '__main__':
    models = [
        'qwen/Qwen-7B-Chat',
        'damo/nlp_gte_sentence-embedding_chinese-base',
    ]
    
    cache_dir = './models'
    os.makedirs(cache_dir, exist_ok=True)
    
    for model_name in models:
        print(f"\n{'='*60}")
        download_model(model_name, cache_dir=cache_dir)
        print(f"{'='*60}\n")
```

## 与项目集成示例

如果您需要在 RAG 系统中使用 ModelScope 下载的模型：

```python
from modelscope import snapshot_download
from transformers import AutoModel, AutoTokenizer

# 1. 下载模型
model_dir = snapshot_download(
    'qwen/Qwen-7B-Chat',
    cache_dir='./models'
)

# 2. 加载模型
model = AutoModel.from_pretrained(
    model_dir,
    trust_remote_code=True,
    device_map='auto'  # 自动分配设备
)

tokenizer = AutoTokenizer.from_pretrained(
    model_dir,
    trust_remote_code=True
)

# 3. 使用模型
text = "你好，请介绍一下自己"
inputs = tokenizer(text, return_tensors='pt')
outputs = model.generate(**inputs, max_length=100)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## 推荐的模型列表

### 大语言模型
- `qwen/Qwen-7B-Chat` - Qwen 7B 对话模型（推荐）
- `qwen/Qwen-14B-Chat` - Qwen 14B 对话模型
- `qwen/Qwen-1_8B` - Qwen 1.8B 基础模型（轻量级）
- `ZhipuAI/chatglm3-6b` - ChatGLM3 6B 模型

### 嵌入模型
- `damo/nlp_gte_sentence-embedding_chinese-base` - 中文基础嵌入模型
- `damo/nlp_gte_sentence-embedding_chinese-large` - 中文大型嵌入模型

### 多模态模型
- `qwen/Qwen-VL-Chat` - Qwen 视觉语言模型

## 更多资源

- [ModelScope 官网](https://www.modelscope.cn/)
- [ModelScope GitHub](https://github.com/modelscope/modelscope)
- [ModelScope 文档](https://modelscope.cn/docs)
- [模型库](https://modelscope.cn/models)

## 注意事项

1. **磁盘空间**：大型模型（如 7B+）可能需要 15GB+ 的磁盘空间
2. **网络要求**：首次下载需要稳定的网络连接
3. **内存要求**：加载模型时需要足够的 RAM（7B 模型建议 16GB+）
4. **许可证**：使用前请查看模型的许可证要求
5. **版本兼容性**：某些模型可能需要特定版本的 transformers 或其他依赖

---

**提示**：如果您在使用过程中遇到问题，可以查看 ModelScope 官方文档或在 GitHub 上提交 Issue。
